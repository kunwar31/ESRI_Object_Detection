{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fd9570466736b09c16d4ed020cb81aef494753d"
   },
   "source": [
    "# **ESRI DATA SCIENCE CHALLENGE 2019**\n",
    "\n",
    "\n",
    "In this challenge, we have been given 224x224 images containing cars and swimming pools labelled in PASCAL VOC Format. As we'll be using RetinaNet for this challenge, we'll make use of keras-retinanet package from github. \n",
    "* [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) - Research paper describing RetinaNet and Focal Loss which it uses\n",
    "* [keras-retinanet](https://github.com/fizyr/keras-retinanet) - Keras RetinaNet implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/fizyr/keras-retinanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip3 install keras-retinanet/ --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb2593dea04ab0e20ef4ee521dd8cba7f3bd987b"
   },
   "outputs": [],
   "source": [
    "!rm keras-retinanet/ -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "907236db932c74a1170900b8e6d9cc60bd086206"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from os import listdir, walk\n",
    "from os.path import join\n",
    "from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n",
    "from keras_retinanet.models import backbone,load_model,convert_model\n",
    "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
    "from keras_retinanet.utils.visualization import draw_boxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "faef2a3eac3226eb2e8211a9c81f89b9a6a811b3"
   },
   "outputs": [],
   "source": [
    "classes = ['1','2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56a388a1b5fc53f9ffd6c207a87bae5b8ffbc543"
   },
   "source": [
    "# Load and Convert Annotations\n",
    "\n",
    "Here we load annotations given in PASCAL VOC Format and save them in CSV Format as required by keras-retinanet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7673f02c16e5bb5e3734f23d2a533326b9fba562"
   },
   "outputs": [],
   "source": [
    "def convert_annotation(image_id,filename):\n",
    "    in_file = open('training_data/labels/%s.xml'%(image_id))\n",
    "    out_file = open(filename, 'a')\n",
    "    tree=ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    if root.iter('object') is not None:\n",
    "        for obj in root.iter('object'):\n",
    "            cls = obj.find('name').text\n",
    "            if cls not in classes:\n",
    "                continue\n",
    "            cls_id = classes.index(cls)\n",
    "            \n",
    "            xmlbox = obj.find('bndbox')\n",
    "            x1 = math.ceil(float(xmlbox.find('xmin').text))\n",
    "            y1 = math.ceil(float(xmlbox.find('ymin').text))\n",
    "            x2 = math.ceil(float(xmlbox.find('xmax').text))\n",
    "            y2 = math.ceil(float(xmlbox.find('ymax').text))\n",
    "            if x1 == x2 or y1 == y2:\n",
    "                continue\n",
    "                \n",
    "            out_file.write(f'training_data/images/{image_id}.jpg,{x1},{y1},{x2},{y2},{cls}\\n')\n",
    "    else:\n",
    "        out_file.write(f'training_data/images/{image_id}.jpg,,,,,\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04461e1dd8304d4224c0ce6fe9a1da1040b8aa9d"
   },
   "source": [
    "# Training and Validation split\n",
    "\n",
    "Normally we would have 10-30% of our images in validation set but as we want best possible score we'll use all our images to train, as we have quite few training images already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32f27dd808794bf8659b9e668b0e490bc6a104b7"
   },
   "outputs": [],
   "source": [
    "_,_,image_ids = next(walk('training_data/images'))\n",
    "image_ids = [i[:-4] for i in image_ids]\n",
    "open('annotations.csv','w')\n",
    "open('val_annotations.csv','w')\n",
    "\n",
    "train_ids,val_ids = train_test_split(image_ids,random_state=31,test_size=0)\n",
    "\n",
    "for image_id in train_ids:\n",
    "    convert_annotation(image_id,'annotations.csv')\n",
    "        \n",
    "for image_id in val_ids:\n",
    "    convert_annotation(image_id,'val_annotations.csv')\n",
    "    \n",
    "print(len(train_ids),len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93124e395a1fdc94926afe1b71179770c84d587b"
   },
   "outputs": [],
   "source": [
    "with open('classes.csv','w') as f:\n",
    "    f.write('1,0\\n2,1\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "add87f5c791fbe8da4793a384e983235587d1626"
   },
   "source": [
    "# Anchor Parameters\n",
    "\n",
    "1. Anchor parameters are used to decide how anchor boxes will be generated for the model.\n",
    "1. As we're dealing mostly small boxes with can be highly elongated, we'll change ratios and scales to fit our needs.\n",
    "1. test_anchors.ipynb is used to visualize anchors on ground truth boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17398f319e28dd6e38948e2e09dc87481e1966bc"
   },
   "outputs": [],
   "source": [
    "with open('config.ini','w') as f:\n",
    "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.25 0.5 0.75 1 1.5 2 4 6 8 10\\nscales  = 0.5 1 2\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867d2c56814d761227270459f837b787e616f4c5"
   },
   "source": [
    "# Some Hyperparameters\n",
    "\n",
    "We will rescale our images to 672x672 for better precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab8e28dc982006ee8f8800a9021433f0d716fc48"
   },
   "outputs": [],
   "source": [
    "b = backbone('resnet50')\n",
    "\n",
    "class args:\n",
    "    batch_size = 64\n",
    "    config = read_config_file('config.ini')\n",
    "    random_transform = True # Image augmentation\n",
    "    annotations = 'annotations.csv'\n",
    "    val_annotations = 'val_annotations.csv'\n",
    "    classes = 'classes.csv'\n",
    "    image_min_side = 672\n",
    "    image_max_side = 672\n",
    "    dataset_type = 'csv'\n",
    "    tensorboard_dir = ''\n",
    "    evaluation = False\n",
    "    snapshots = True\n",
    "    snapshot_path = \"saved/\"\n",
    "    backbone = 'resnet50'\n",
    "    epochs = 70\n",
    "    steps = len(train_ids)//(batch_size)\n",
    "    weighted_average = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4551ed97d6ee33f91283f559bbb7d9549d2d1f6"
   },
   "outputs": [],
   "source": [
    "train_gen,valid_gen = create_generators(args,b.preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48482aa28034ede1fc5dd69661bbf83163ddb7bd"
   },
   "source": [
    "# Image Augmentation\n",
    "\n",
    "In addition to augmentations already done by keras-retinanet [here](https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py#L227) , we'll use a package called imgaug to furthur augment the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c09f817a02a3c385181a216f1c7c1af993efcf09"
   },
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "# Define our sequence of augmentation steps that will be applied to every image.\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        #\n",
    "        # Execute 1 to 9 of the following (less important) augmenters per\n",
    "        # image. Don't execute all of them, as that would often be way too\n",
    "        # strong.\n",
    "        #\n",
    "        iaa.SomeOf((1, 9),\n",
    "            [\n",
    "\n",
    "                        # Blur each image with varying strength using\n",
    "                        # gaussian blur (sigma between 0 and .5),\n",
    "                        # average/uniform blur (kernel size 1x1)\n",
    "                        # median blur (kernel size 1x1).\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0,0.5)),\n",
    "                            iaa.AverageBlur(k=(1)),\n",
    "                            iaa.MedianBlur(k=(1)),\n",
    "                        ]),\n",
    "\n",
    "                        # Sharpen each image, overlay the result with the original\n",
    "                        # image using an alpha between 0 (no sharpening) and 1\n",
    "                        # (full sharpening effect).\n",
    "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
    "\n",
    "                        # Add gaussian noise to some images.\n",
    "                        # In 50% of these cases, the noise is randomly sampled per\n",
    "                        # channel and pixel.\n",
    "                        # In the other 50% of all cases it is sampled once per\n",
    "                        # pixel (i.e. brightness change).\n",
    "                        iaa.AdditiveGaussianNoise(\n",
    "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
    "                        ),\n",
    "\n",
    "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
    "                        # them to black) or drop them on an image with 2-5% percent\n",
    "                        # of the original size, leading to large dropped\n",
    "                        # rectangles.\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
    "                            iaa.CoarseDropout(\n",
    "                                (0.03, 0.15), size_percent=(0.02, 0.05),\n",
    "                                per_channel=0.2\n",
    "                            ),\n",
    "                        ]),\n",
    "\n",
    "                        # Add a value of -5 to 5 to each pixel.\n",
    "                        iaa.Add((-5, 5), per_channel=0.5),\n",
    "\n",
    "                        # Change brightness of images (85-115% of original value).\n",
    "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
    "\n",
    "                        # Improve or worsen the contrast of images.\n",
    "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
    "\n",
    "                        # Convert each image to grayscale and then overlay the\n",
    "                        # result with the original with random alpha. I.e. remove\n",
    "                        # colors with varying strengths.\n",
    "                        iaa.Grayscale(alpha=(0.0, 0.25)),\n",
    "\n",
    "                        # In some images distort local areas with varying strength.\n",
    "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
    "                    ],\n",
    "            # do all of the above augmentations in random order\n",
    "            random_order=True\n",
    "        )\n",
    "    ],\n",
    "    # do all of the above augmentations in random order\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2743e4a77f3b2d2e1c702b0c968e07e7603d2cc"
   },
   "outputs": [],
   "source": [
    "def augment_train_gen(train_gen,visualize=False):\n",
    "    '''\n",
    "    Creates a generator using another generator with applied image augmentation.\n",
    "    Args\n",
    "        train_gen  : keras-retinanet generator object.\n",
    "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
    "    '''\n",
    "    imgs = []\n",
    "    boxes = []\n",
    "    targets = []\n",
    "    size = train_gen.size()\n",
    "    idx = 0\n",
    "    while True:\n",
    "        while len(imgs) < args.batch_size:\n",
    "            image       = train_gen.load_image(idx % size)\n",
    "            annotations = train_gen.load_annotations(idx % size)\n",
    "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
    "            imgs.append(image)            \n",
    "            boxes.append(annotations['bboxes'])\n",
    "            targets.append(annotations)\n",
    "            idx += 1\n",
    "        if visualize:\n",
    "            imgs = seq.augment_images(imgs)\n",
    "            imgs = np.array(imgs)\n",
    "            boxes = np.array(boxes)\n",
    "            yield imgs,boxes\n",
    "        else:\n",
    "            imgs = seq.augment_images(imgs)\n",
    "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
    "            imgs = train_gen.compute_inputs(imgs)\n",
    "            targets = train_gen.compute_targets(imgs,targets)\n",
    "            imgs = np.array(imgs)\n",
    "            yield imgs,targets\n",
    "        imgs = []\n",
    "        boxes = []\n",
    "        targets = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "755f6bb05e3251d40877ddba74c8f6486678f47c"
   },
   "source": [
    "# Visualize augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcd7d99112b845fe8b42ca980d3842361db93614"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skip_batches = 5\n",
    "i = 0\n",
    "\n",
    "for imgs,boxes in augment_train_gen(train_gen,visualize=True):\n",
    "    if i > skip_batches:\n",
    "        fig=plt.figure(figsize=(24,96))\n",
    "        columns = 2\n",
    "        rows = 8\n",
    "        for i in range(1, columns*rows + 1):\n",
    "            draw_boxes(imgs[i], boxes[i], (0, 255, 0), thickness=1)\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35f45954269581be05dbc6a9601965db625609d8"
   },
   "source": [
    "# More Hyperparameters\n",
    "\n",
    "we'll use learning rate of 0.001 and freeze weights for the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de392d655a630d3b73dfa84fe1febbed0f3fdbe2"
   },
   "outputs": [],
   "source": [
    "model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=b.retinanet,\n",
    "            num_classes=train_gen.num_classes(),\n",
    "            weights=None,\n",
    "            multi_gpu=False,\n",
    "            freeze_backbone=True,\n",
    "            lr=1e-3,\n",
    "            config=args.config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a34e90ba8fec837af9007610241b487b9a6a807c"
   },
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(\n",
    "    model,\n",
    "    training_model,\n",
    "    prediction_model,\n",
    "    valid_gen,\n",
    "    args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33285a3b74dc8580797f86678e080cbfecd7c668"
   },
   "source": [
    "# Download pretrained model\n",
    "\n",
    "We download a pretrained model on COCO dataset and load it's weights, we'll skip loading the weights for the few last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "764a0430cc7c5f5f341bb3ba3bb89b699248481e"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.0/resnet50_coco_best_v2.1.0.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "275ddf407b82092a0ec8a514500c0987f918d59b"
   },
   "outputs": [],
   "source": [
    "training_model.load_weights('resnet50_coco_best_v2.1.0.h5',skip_mismatch=True,by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7cd58d21715eb0ec3a9de5b30be8cea85915726"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "We will train for 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06bca3482f679c50640b7396bafb9b291b4f402b"
   },
   "outputs": [],
   "source": [
    "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
    "        steps_per_epoch=args.steps,\n",
    "        epochs=args.epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a5b84c37c58444c37e6d08fb6de19b756c03926"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
